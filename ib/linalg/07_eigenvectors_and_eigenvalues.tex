\subsection{Eigenvalues}
Let \( V \) be an \( F \)-vector space.
Let \( \dim V = n < \infty \), and let \( \alpha \) be an endomorphism of \( V \).
We wish to find a basis \( B \) of \( V \) such that, in this basis, \( [\alpha]_B \equiv [\alpha]_{B,B} \) has a simple (e.g.\ diagonal, triangular) form.
Recall that if \( B' \) is another basis and \( P \) is the change of basis matrix, \( [\alpha]_{B'} = P^{-1} [\alpha]_B P \).
Equivalently, given a square matrix \( A \in M_n(F) \) we want to conjugate it by a matrix \( P \) such that the result is `simpler'.
\begin{definition}
	Let \( \alpha \in L(V) \) be an endomorphism.
	We say that \( \alpha \) is \textit{diagonalisable} if there exists a basis \( B \) of \( V \) such that the matrix \( [\alpha]_B \) is diagonal.
	We say that \( \alpha \) is \textit{triangulable} if there exists a basis \( B \) of \( V \) such that \( [\alpha]_B \) is triangular.
\end{definition}
\begin{remark}
	We can express this equivalently in terms of conjugation of matrices.
\end{remark}
\begin{definition}
	A scalar \( \lambda \in F \) is an \textit{eigenvalue} of an endomorphism \( \alpha \) if and only if there exists a vector \( v \in V \setminus \qty{0} \) such that \( \alpha(v) = \lambda v \).
	Such a vector is an \textit{eigenvector} with eigenvalue \( \lambda \).
	\( V_\lambda = \qty{ v \in V \colon \alpha(v) = \lambda v } \leq V \) is the \textit{eigenspace} associated to \( \lambda \).
\end{definition}
\begin{lemma}
	\( \lambda \) is an eigenvalue if and only if \( \det(\alpha - \lambda I) = 0 \).
\end{lemma}
\begin{proof}
	If \( \lambda \) is an eigenvalue, there exists a nonzero vector \( v \) such that \( \alpha(v) = \lambda v \), so \( (\alpha - \lambda)(v) = 0 \).
	So the kernel is non-trivial.
	So \( \alpha - \lambda I \) is not injective, so it is not surjective by the rank-nullity theorem.
	Hence this matrix is not invertible, so it has zero determinant.
\end{proof}
\begin{remark}
	If \( \alpha(v_j) = \lambda v_j \) for \( j \in \qty{1, \dots, m} \), we can complete the family \( v_j \) into a basis \( (v_1, \dots, v_n) \) of \( V \).
	Then in this basis, the first \( m \) columns of the matrix \( \alpha \) has diagonal entries \( \lambda_j \).
\end{remark}

\subsection{Polynomials}
Recall the following facts about polynomials on a field, for instance
\[
	f(t) = a_n t^n + \dots + a_1 t + a_0
\]
We say that the degree of \( f \), written \( \deg f \) is \( n \).
The degree of \( f + g \) is at most the maximum degree of \( f \) and \( g \).
\( \deg (fg) = \deg f + \deg g \).
Let \( F[t] \) be the vector space of polynomials with coefficients in \( F \).
If \( \lambda \) is a root of \( f \), then \( (t-\lambda) \) divides \( F \).
\begin{proof}
	\[
		f(t) = a_n t^n + \dots + a_1 t + a_0
	\]
	Hence,
	\[
		f(\lambda) = a_n \lambda^n + \dots + a_1 \lambda + a_0 = 0
	\]
	which implies that
	\[
		f(t) = f(t) - f(\lambda) = a_n(t^n - \lambda^n) + \dots + a_1(t - \lambda)
	\]
	But note that, for all \( n \),
	\[
		t^n - \lambda^n = (1-\lambda)(t^{n-1} + \lambda t^{n-2} + \dots + \lambda^{n-2} t + \lambda^{n-1})
	\]
\end{proof}
\begin{remark}
	We say that \( \lambda \) is a root of \textit{multiplicity} \( k \) if \( (t-\lambda)^k \) divides \( f \) but \( (t-\lambda)^{k+1} \) does not.
\end{remark}
\begin{corollary}
	A nonzero polynomial of degree \( n \) has at most \( n \) roots, counted with multiplicity.
\end{corollary}
\begin{corollary}
	If \( f_1, f_2 \) are two polynomials of degree less than \( n \) such that \( f_1(t_i) = f_2(t_i) \) for \( i \in \qty{1, \dots, n} \) and \( t_i \) distinct, then \( f_1 \equiv f_2 \).
\end{corollary}
\begin{proof}
	\( f_1 - f_2 \) has degree less than \( n \), but has \( n \) roots.
	Hence it is zero.
\end{proof}
\begin{theorem}
	Any polynomial \( f \in \mathbb C[t] \) of positive degree has a complex root.
	When counted with multiplicity, \( f \) has a number of roots equal to its degree.
\end{theorem}
\begin{corollary}
	Any polynomial \( f \in \mathbb C[t] \) can be factorised into an amount of linear factors equal to its degree.
\end{corollary}

\subsection{Characteristic polynomials}
\begin{definition}
	Let \( \alpha \) be an endomorphism.
	The \textit{characteristic polynomial} of \( \alpha \) is
	\[
		\chi_\alpha(\lambda) = \det(\alpha - \lambda I)
	\]
\end{definition}
\begin{remark}
	\( \chi_\alpha \) is a polynomial because the determinant is defined as a polynomial in the terms of the matrix.
	Note further that conjugate matrices have the same characteristic polynomial, so the above definition is well defined in any basis.
	Indeed, \( \det(P^{-1}\alpha P - \lambda I) = \det(P^{-1}(\alpha - \lambda I)P) = \det(\alpha - \lambda I) \).
\end{remark}
\begin{theorem}
	Let \( \alpha \in L(V) \).
	\( \alpha \) is triangulable if and only if \( \chi_\alpha \) can be written as a product of linear factors over \( F \).
	In particular, all complex matrices are triangulable.
\end{theorem}
\begin{proof}
	Suppose \( \alpha \) is triangulable.
	Then for a basis \( B \), \( [\alpha]_B \) is triangulable with diagonal entries \( a_i \).
	Then
	\[
		\chi_\alpha(t) = (a_1 - t)(a_2 - t) \cdots (a_n - t)
	\]
	Conversely, let \( \chi_\alpha(t) \) be the characteristic polynomial of \( \alpha \) with a root \( \lambda \).
	Then, \( \chi_\alpha(\lambda) = 0 \) implies \( \lambda \) is an eigenvalue.
	Let \( V_\lambda \) be the corresponding eigenspace.
	Let \( (v_1, \dots, v_k) \) be the basis of this eigenspace, completed to a basis \( (v_1, \dots, v_n) \) of \( V \).
	Let \( W = \vecspan\qty{v_{k+1}, \dots, v_n} \), and then \( V = V_\lambda \oplus W \).
	Then
	\[
		[\alpha]_B = \begin{pmatrix}
			\lambda I & \star \\
			0         & C
		\end{pmatrix}
	\]
	where \( \star \) is arbitrary, and \( C \) is a block of size \( (n-k) \times (n-k) \).
	Then \( \alpha \) induces an endomorphism \( \overline \alpha \colon V/U \to V/U \) with respect to the basis \( (v_{k+1}, \dots, v_n) \).
	By induction on the dimension, we can find a basis \( (w_{k+1}, \dots, w_n) \) for which \( C \) has a triangular form.
	Then the basis \( (v_1, \dots, v_k, w_{k+1}, \dots, w_n) \) is a basis for which \( \alpha \) is triangular.
\end{proof}
\begin{lemma}
	Let \( n = \dim V \), and \( V \) be a vector space over \( \mathbb R \) or \( \mathbb C \).
	Let \( \alpha \) be an endomorphism on \( V \).
	Then
	\[
		\chi_\alpha(t) = (-1)^n t^n + c_{n-1} t^{n-1} + \dots + c_0
	\]
	with
	\[
		c_0 = \det A;\quad c_{n-1} = (-1)^{n-1} \tr A
	\]
\end{lemma}
\begin{proof}
	\[
		\chi_\alpha(t) = \det(\alpha - t I) \implies \chi_\alpha(0) = \det(\alpha)
	\]
	Further, for \( \mathbb R, \mathbb C \) we know that \( \alpha \) is triangulable over \( \mathbb C \).
	Hence \( \chi_\alpha(t) \) is the determinant of a triangular matrix;
	\[
		\chi_\alpha(t) = \prod_{i=1}^n (a_i - t)
	\]
	Hence
	\[
		c_{n-1} = (-1)^{n-1} a_i
	\]
	Since the trace is invariant under a change of basis, this is exactly the trace as required.
\end{proof}

\subsection{Polynomials for matrices and endomorphisms}
Let \( p(t) \) be a polynomial over \( F \).
We will write
\[
	p(t) = a_n t^n + \dots + a_0
\]
For a matrix \( A \in M_n(F) \), we write
\[
	p(A) = a_n A^n + \dots + a_0 \in M_n(F)
\]
For an endomorphism \( \alpha \in L(V) \),
\[
	p(\alpha) = a_n \alpha^n + \dots + a_0 I \in L(V);\quad \alpha^k \equiv \underbrace{\alpha \circ \dots \circ \alpha}_{k \text{ times}}
\]

\subsection{Sharp criterion of diagonalisability}
\begin{theorem}
	Let \( V \) be a vector space over \( F \) of finite dimension \( n \).
	Let \( \alpha \) be an endomorphism of \( V \).
	Then \( \alpha \) is diagonalisable if and only if there exists a polynomial \( p \) which is a product of \textit{distinct} linear factors, such that \( p(\alpha) = 0 \).
	In other words, there exist distinct \( \lambda_1, \dots, \lambda_k \) such that
	\[
		p(t) = \prod_{i=1}^n (t - \lambda_i) \implies p(\alpha) = 0
	\]
\end{theorem}
\begin{proof}
	Suppose \( \alpha \) is diagonalisable in a basis \( B \).
	Let \( \lambda_1, \dots, \lambda_k \) be the \( k \leq n \) \textit{distinct} eigenvalues.
	Let
	\[
		p(t) = \prod_{i=1}^k (t-\lambda_i)
	\]
	Let \( v \in B \).
	Then \( \alpha(v) = \lambda_i v \) for some \( i \).
	Then, since the terms in the following product commute,
	\[
		(\alpha - \lambda_i I)(v) = 0 \implies p(\alpha)(v) = \qty[\prod_{i=1}^k (\alpha - \lambda_i I)](v) = 0
	\]
	So for all basis vectors, \( p(\alpha)(v) \).
	By linearity, \( p(\alpha) = 0 \).

	Conversely, suppose that \( p(\alpha) = 0 \) for some polynomial \( p(t) = \prod_{i=1}^k (t-\lambda_i) \) with distinct \( \lambda_i \).
	Let \( V_{\lambda_i} = \ker(\alpha - \lambda_i I) \).
	We claim that
	\[
		V = \bigoplus_{i=1}^k V_{\lambda_i}
	\]
	Consider the polynomials
	\[
		q_j(t) = \prod_{i=1,i \neq j}^k \frac{t-\lambda_i}{\lambda_j - \lambda_i}
	\]
	These polynomials evaluate to one at \( \lambda_j \) and zero at \( \lambda_i \) for \( i \neq j \).
	Hence \( q_j(\lambda_i) = \delta_{ij} \).
	We now define the polynomial
	\[
		q = q_1 + \dots + q_k
	\]
	The degree of \( q \) is at most \( (k-1) \).
	Note, \( q(\lambda_i) = 1 \) for all \( i \in \qty{1, \dots, k} \).
	The only polynomial that evaluates to one at \( k \) points with degree at most \( (k-1) \) is exactly given by \( q(t) = 1 \).
	Consider the endomorphism
	\[
		\pi_j = q_j(\alpha) \in L(V)
	\]
	These are called the `projection operators'.
	By construction,
	\[
		\sum_{j=1}^k \pi_j = \sum_{j=1}^k q_j(\alpha) = I
	\]
	So the sum of the \( \pi_j \) is the identity.
	Hence, for all \( v \in V \),
	\[
		I(v) = v = \sum_{j=1}^k \pi_j(v) = \sum_{j=1}^k q_j(\alpha)(v)
	\]
	So we can decompose any vector as a sum of its projections \( \pi_j(v) \).
	Now, by definition of \( q_j \) and \( p \),
	\begin{align*}
		(\alpha - \lambda_j I) q_j(\alpha)(v) & = \frac{1}{\prod_{i \neq j} (\lambda_j - \lambda_i)} (\alpha - \lambda_j I) \qty[\prod_{i \neq j} (t - \lambda_i)] (\alpha) \\
		                                      & = \frac{1}{\prod_{i \neq j} (\lambda_j - \lambda_i)} \prod_{i=1}^k (\alpha - \lambda_i I)(v)                                \\
		                                      & = \frac{1}{\prod_{i \neq j} (\lambda_j - \lambda_i)} p(\alpha)(v)
	\end{align*}
	By assumption, this is zero.
	For all \( v \), we have \( (\alpha - \lambda_j I) q_j(\alpha)(v) \).
	Hence,
	\[
		(\alpha - \lambda_j I) \pi_j(v) = 0 \implies \pi_j(v) \in \ker(\alpha - \lambda_j I) = v_j
	\]
	We have then proven that, for all \( v \in V \),
	\[
		v = \sum_{j=1}^k \underbrace{\pi_j(v)}_{\in V_j}
	\]
	Hence,
	\[
		V = \sum_{j=1}^k V_j
	\]
	It remains to show that the sum is direct.
	Indeed, let
	\[
		v \in V_{\lambda_j} \cap \qty(\sum_{i \neq j} V_{\lambda_i})
	\]
	We must show \( v = 0 \).
	Applying \( \pi_j \),
	\[
		\pi_j(v) = q_j(\alpha)(v) = \prod_{i \neq j} \frac{(\alpha - \lambda_i I)(v)}{\lambda_j - \lambda_i}
	\]
	Since \( \alpha(v) = \lambda_j v \),
	\[
		\pi_j(v) = \prod_{i \neq j} \frac{(\lambda_j - \lambda_i)v}{\lambda_j - \lambda_i} = v
	\]
	Hence \( \pi_j \) really projects onto \( V_{\lambda_j} \).
	However, we also know \( v \in \sum_{i \neq j} V_{\lambda_i} \).
	So we can write \( v = \sum_{i \neq j} w_i \) for \( w \in V_{\lambda_i} \).
	Thus,
	\[
		\pi_j(w_i) = \prod_{m \neq j} \frac{(\alpha - \lambda_m I)(v)}{\lambda_m - \lambda_j}
	\]
	Since \( \alpha(w_i) = \lambda_i w_i \), one of the factors will vanish, hence
	\[
		\pi_j(w_i) = 0
	\]
	So
	\[
		v = \sum_{i \neq j} w_i \implies \pi_j(v) = \sum_{i \neq j} \pi_j(w_i) = 0
	\]
	But \( v = \pi_j(v) \) hence \( v = 0 \).
	So the sum is direct.
	Hence, \( B = (B_1, \dots, B_k) \) is a basis of \( V \), where the \( B_i \) are bases of \( V_{\lambda_i} \).
	Then \( [\alpha]_B \) is diagonal.
\end{proof}
\begin{remark}
	We have shown further that if \( \lambda_1, \dots, \lambda_k \) are distinct eigenvalues of \( \alpha \), then
	\[
		\sum_{i=1}^k V_{\lambda_i} = \bigoplus_{i=1}^k V_{\lambda_i}
	\]
	Therefore, the only way that diagonalisation fails is when this sum is not direct, so
	\[
		\sum_{i=1}^k V_{\lambda_i} < V
	\]
\end{remark}
\begin{example}
	Let \( F = \mathbb C \).
	Let \( A \in M_n(F) \) such that \( A \) has finite order; there exists \( m \in \mathbb N \) such that \( A^m = I \).
	Then \( A \) is diagonalisable.
	This is because
	\[
		t^m - 1 = p(t) = \prod_{j=1}^m (t - \xi_m^j);\quad \xi_m = e^{2 \pi i/m}
	\]
	and \( p(A) = 0 \).
\end{example}

\subsection{Simultaneous diagonalisation}
\begin{theorem}
	Let \( \alpha, \beta \) be endomorphisms of \( V \) which are diagonalisable.
	Then \( \alpha, \beta \) are \textit{simultaneously diagonalisable} (there exists a basis \( B \) of \( V \) such that \( [\alpha]_B, [\beta]_B \) are diagonal) if and only if \( \alpha \) and \( \beta \) commute.
\end{theorem}
\begin{proof}
	Two diagonal matrices commute.
	If such a basis exists, \( \alpha \beta = \beta \alpha \) in this basis.
	So this holds in any basis.
	Conversely, suppose \( \alpha \beta = \beta \alpha \).
	We have
	\[
		V = \bigoplus_{i=1}^k V_{\lambda_i}
	\]
	where \( \lambda_i, \dots, \lambda_k \) are the \( k \) distinct eigenvalues of \( \alpha \).
	We claim that \( \beta \qty(V_{\lambda_j}) \leq V_{\lambda_j} \).
	Indeed, for \( v \in V_{\lambda_j} \),
	\[
		\alpha \beta(v) = \beta \alpha(v) = \beta(\lambda_j v) = \lambda_j \beta(v) \implies \alpha(\beta(v)) = \lambda_j \beta(v)
	\]
	Hence, \( \beta(v) \in V_{\lambda_j} \).
	By assumption, \( \beta \) is diagonalisable.
	Hence, there exists a polynomial \( p \) with distinct linear factors such that \( p(\beta) = 0 \).
	Now, \( \beta\qty(V_{\lambda_j}) \leq V_{\lambda_j} \) so we can consider \( \eval{\beta}_{V_{\lambda_j}} \).
	This is an endomorphism of \( V_{\lambda_j} \).
	We can compute
	\[
		p\qty(\eval{\beta}_{V_{\lambda_j}}) = 0
	\]
	Hence, \( \eval{\beta}_{V_{\lambda_j}} \) is diagonalisable.
	Let \( B_i \) be the basis of \( V_{\lambda_i} \) in which \( \eval{\beta}_{V_{\lambda_j}} \) is diagonal.
	Since \( V = \bigoplus V_{\lambda_i} \), \( B = (B_1, \dots, B_k) \) is a basis of \( V \).
	Then the matrices of \( \alpha \) and \( \beta \) in \( V \) are diagonal.
\end{proof}

\subsection{Minimal polynomials}
Recall from IB Groups, Rings and Modules the Euclidean algorithm for dividing polynomials.
Given \( a, b \) polynomials over \( F \) with \( b \) nonzero, there exist polynomials \( q, r \) over \( F \) with \( \deg r < \deg b \) and \( a = qb + r \).
\begin{definition}
	Let \( V \) be a finite dimensional \( F \)-vector space.
	Let \( \alpha \) be an endomorphism on \( V \).
	The \textit{minimal polynomial} \( m_\alpha \) of \( \alpha \) is the nonzero polynomial with smallest degree such that \( m_\alpha(\alpha) = 0 \).
\end{definition}
\begin{remark}
	If \( \dim V = n < \infty \), then \( \dim L(V) = n^2 \).
	In particular, the family \( \qty{I, \alpha, \dots, \alpha^{n^2}} \) cannot be free since it has \( n^2+1 \) entries.
	This generates a polynomial in \( \alpha \) which evaluates to zero.
	Hence, a minimal polynomial always exists.
\end{remark}
\begin{lemma}
	Let \( \alpha \in L(V) \) and \( p \in F[t] \) be a polynomial.
	Then \( p(\alpha) = 0 \) if and only if \( m_\alpha \) is a factor of \( p \).
	In particular, \( m_\alpha \) is well-defined and unique up to a constant multiple.
\end{lemma}
\begin{proof}
	Let \( p \in F[t] \) such that \( p(\alpha) = 0 \).
	If \( m_\alpha(\alpha) = 0 \) and \( \deg m_\alpha < \deg p \), we can perform the division \( p = m_\alpha q + r \) for \( \deg r < \deg m_\alpha \).
	Then \( p(\alpha) = m_\alpha(\alpha) q(\alpha) + r(\alpha) \).
	But \( m_\alpha(\alpha) = 0 \).
	But \( \deg r < \deg m_\alpha \) and \( m_\alpha \) is the smallest degree polynomial which evaluates to zero for \( \alpha \), so \( r \equiv 0 \) so \( p = m_\alpha q \).
	In particular, if \( m_1, m_2 \) are both minimal polynomials that evaluate to zero for \( \alpha \), we have \( m_1 \) divides \( m_2 \) and \( m_2 \) divides \( m_1 \).
	Hence they are equivalent up to a constant.
\end{proof}
\begin{example}
	Let \( V = F^2 \) and
	\[
		A= \begin{pmatrix}
			1 & 0 \\
			0 & 1
		\end{pmatrix};\quad B = \begin{pmatrix}
			1 & 1 \\
			0 & 1
		\end{pmatrix}
	\]
	We can check \( p(t) = (t-1)^2 \) gives \( p(A) = p(B) = 0 \).
	So the minimal polynomial of \( A \) or \( B \) must be either \( (t-1) \) or \( (t-1)^2 \).
	For \( A \), we can find the minimal polynomial is \( (t-1) \), and for \( B \) we require \( (t-1)^2 \).
	So \( B \) is not diagonalisable, since its minimal polynomial is not a product of distinct linear factors.
\end{example}

\subsection{Cayley-Hamilton theorem}
\begin{theorem}
	Let \( V \) be a finite dimensional \( F \)-vector space.
	Let \( \alpha \in L(V) \) with characteristic polynomial \( \chi_\alpha(t) = \det(\alpha - t I) \).
	Then \( \chi_\alpha(\alpha) = 0 \).
\end{theorem}
\noindent Two proofs will provided; one more physical and based on \( F = \mathbb C \) and one more algebraic.
\begin{proof}
	Let \( B = \qty{v_1, \dots, v_n} \) be a basis of \( V \) such that \( [\alpha]_B \) is triangular.
	This can be done when \( F = \mathbb C \).
	Note, if the diagonal entries in this basis are \( a_i \),
	\[
		\chi_\alpha(t) = \prod_{i=1}^n (a_i - t) \implies \chi_\alpha(\alpha) = (\alpha - a_1 I) \dots (\alpha - a_n I)
	\]
	We want to show that this expansion evaluates to zero.
	Let \( U_j = \vecspan \qty{v_1, \dots, v_j} \).
	Let \( v \in V = U_n \).
	We want to compute \( \chi_\alpha(\alpha)(v) \).
	Note, by construction of the triangular matrix.
	\begin{align*}
		\chi_\alpha(\alpha)(v) & = (\alpha - a_1 I) \dots \underbrace{(\alpha - a_n I)(v)}_{\in U_{n-1}}                     \\
		                       & = (\alpha - a_1 I) \dots \underbrace{(\alpha - a_{n-1} I)(\alpha - a_n I)(v)}_{\in U_{n-2}} \\
		                       & = \dots                                                                                     \\
		                       & \in U_0
	\end{align*}
	Hence this evaluates to zero.
\end{proof}
\noindent The following proof works for any field where we can equate coefficients, but is much less intuitive.
\begin{proof}
	We will write
	\[
		\det(t I - \alpha) = (-1)^n \chi_\alpha(t) = t^n + a_{n-1}t^{n-1} + \dots + a_0
	\]
	For any matrix \( B \), we have proven \( B \adj B = (\det B) I \).
	We apply this relation to the matrix \( B = tI - A \).
	We can check that
	\[
		\adj B = \adj(tI - A) = B_{n-1} t^{n-1} + \dots + B_1 t + B_0
	\]
	since adjugate matrices are degree \( (n-1) \) polynomials for each element.
	Then, by applying \( B \adj B = (\det B) I \),
	\[
		(tI - A) [ B_{n-1} t^{n-1} + \dots + B_1 t + B_0 ] = (\det B) I = (t^n + \dots + a_0) I
	\]
	Since this is true for all \( t \), we can equate coefficients.
	This gives
	\begin{align*}
		t^n     & :      & I         & = B_{n-1}            \\
		t^{n-1} & :      & a_{n-1} I & = B_{n-2} - AB_{n-1} \\
		        & \vdots &           & \vdots               \\
		t^0     & :      & a_0 I     & = -A B_1
	\end{align*}
	Then, substituting \( A \) for \( t \) in each relation will give, for example, \( A^n I = A^n B_{n-1} \).
	Computing the sum of all of these identities, we recover the original polynomial in terms of \( A \) instead of in terms of \( t \).
	Many terms will cancel since the sum telescopes, yielding
	\[
		A^n + a_{n-1} A^{n-1} + \dots + a_0 I = 0
	\]
\end{proof}

\subsection{Algebraic and geometric multiplicity}
\begin{definition}
	Let \( V \) be a finite dimensional \( F \)-vector space.
	Let \( \alpha \in L(V) \) and let \( \lambda \) be an eigenvalue of \( \alpha \).
	Then
	\[
		\chi_\alpha(t) = (t-\lambda)^{a_\lambda} q(t)
	\]
	where \( q(t) \) is a polynomial over \( F \) such that \( (t-\lambda) \) does not divide \( q \).
	\( a_\lambda \) is known as the \textit{algebraic multiplicity} of the eigenvalue \( \lambda \).
	We define the \textit{geometric multiplicity} \( g_\lambda \) of \( \lambda \) to be the dimension of the eigenspace associated with \( \lambda \), so \( g_\lambda = \dim \ker (\alpha - \lambda I) \).
\end{definition}
\begin{lemma}
	If \( \lambda \) is an eigenvalue of \( \alpha \in L(V) \), then \( 1 \leq g_\lambda \leq a_\lambda \).
\end{lemma}
\begin{proof}
	We have \( g_\lambda = \dim \ker (\alpha - \lambda I) \).
	There exists a nontrivial vector \( v \in V \) such that \( v \in \ker(\alpha - \lambda I) \) since \( \lambda \) is an eigenvalue.
	Hence \( g_\lambda \geq 1 \).
	We will show that \( g_\lambda \leq a_\lambda \).
	Indeed, let \( v_1, \dots, v_{g_\lambda} \) be a basis of \( V_\lambda \equiv \ker (\alpha - \lambda I) \).
	We complete this into a basis \( B \equiv \qty(v_1, \dots, v_{g_\lambda}, v_{g_\lambda + 1}, \dots, v_n) \) of \( V \).
	Then note that
	\[
		[\alpha]_B = \begin{pmatrix}
			\lambda I_{g_\lambda} & \star \\
			0                     & A_1
		\end{pmatrix}
	\]
	for some matrix \( A_1 \).
	Now,
	\[
		\det (\alpha - tI) = \det \begin{pmatrix}
			(\lambda - t) I_{g_\lambda} & \star     \\
			0                           & A_1 - t I
		\end{pmatrix}
	\]
	By the formula for determinants of block matrices with a zero block on the off diagonal,
	\[
		\det (\alpha - tI) = (\lambda-t)^{g_\lambda} \det(A_1 - t I)
	\]
	Hence \( g_\lambda \leq a_\lambda \) since the determinant is a polynomial that could have more factors of the same form.
\end{proof}
\begin{lemma}
	Let \( V \) be a finite dimensional \( F \)-vector space.
	Let \( \alpha \in L(V) \) and let \( \lambda \) be an eigenvalue of \( \alpha \).
	Let \( c_\lambda \) be the multiplicity of \( \lambda \) as a root of the minimal polynomial of \( \alpha \).
	Then \( 1 \leq c_\lambda \leq a_\lambda \).
\end{lemma}
\begin{proof}
	By the Cayley-Hamilton theorem, \( \chi_\alpha(\alpha) = 0 \).
	Since \( m_\alpha \) is linear, \( m_\alpha \) divides \( \chi_\alpha \).
	Hence \( c_\lambda \leq a_\lambda \).
	Now we show \( c_\lambda \geq 1 \).
	Indeed, \( \lambda \) is an eigenvalue hence there exists a nonzero \( v \in V \) such that \( \alpha(v) = \lambda v \).
	For such an eigenvector, \( \alpha^P(v) = \lambda^P v \) for \( P \in \mathbb N \).
	Hence for \( p \in F[t] \), \( p(\alpha)(v) = [p(\lambda)](v) \).
	Hence \( m_\alpha(\alpha)(v) = [m_\alpha(\lambda)](v) \).
	Since the left hand side is zero, \( m_\alpha(\lambda) = 0 \).
	So \( c_\lambda \geq 1 \).
\end{proof}
\begin{example}
	Let
	\[
		A = \begin{pmatrix}
			1 & 0 & -2 \\
			0 & 1 & 1  \\
			0 & 0 & 2
		\end{pmatrix}
	\]
	The minimal polynomial can be computed by considering the characteristic polynomial
	\[
		\chi_A(t) = (t-1)^2(t-2)
	\]
	So the minimal polynomial is either \( (t-1)^2(t-2) \) or \( (t-1)(t-2) \)
	We check \( (t-1)(t-2) \).
	\( (A - I)(A - 2I) \) can be found to be zero.
	So \( m_A(t) = (t-1)(t-2) \).
	Since this is a product of distinct linear factors, \( A \) is diagonalisable.
\end{example}
\begin{example}
	Let \( A \) be a Jordan block of size \( n \geq 2 \).
	Then \( g_\lambda = 1 \), \( a_\lambda = n \), and \( c_\lambda = n \).
\end{example}

\subsection{Characterisation of diagonalisable complex endomorphisms}
\begin{lemma}
	Let \( F = \mathbb C \).
	Let \( V \) be a finite-dimensional \( \mathbb C \)-vector space.
	Let \( \alpha \) be an endomorphism of \( V \).
	Then the following are equivalent.
	\begin{enumerate}
		\item \( \alpha \) is diagonalisable;
		\item for all \( \lambda \) eigenvalues of \( \alpha \), we have \( a_\lambda = g_\lambda \);
		\item for all \( \lambda \) eigenvalues of \( \alpha \), \( c_\lambda = 1 \).
	\end{enumerate}
\end{lemma}
\begin{proof}
	First, the fact that (i) is true if and only if (iii) is true has already been proven.
	Now let us show that (i) is equivalent to (ii).
	Let \( \lambda_1, \dots, \lambda_k \) be the distinct eigenvalues of \( \alpha \).
	We have already found that \( \alpha \) is diagonalisable if and only if \( V = \bigoplus V_{\lambda_i} \).
	The sum was found to be always direct, regardless of diagonalisability.
	We will compute the dimension of \( V \) in two ways;
	\[
		n = \dim V = \deg \chi_\alpha;\quad n = \dim V = \sum_{i=1}^k a_{\lambda_i}
	\]
	since \( \chi_\alpha \) is a product of \( (t-\lambda_i) \) factors as \( F = \mathbb C \).
	Since the sum is direct,
	\[
		\dim \qty(\bigoplus_{i=1}^k V_{\lambda_i}) = \sum_{i=1}^k g_{\lambda_i}
	\]
	\( \alpha \) is diagonalisable if and only if the dimensions are equal, so
	\[
		\sum_{i=1}^k g_{\lambda_i} = \sum_{i=1}^k a_{\lambda_i}
	\]
	Conversely, we have proven that for all eigenvalues \( \lambda_i \), we have \( g_{\lambda_i} \leq a_{\lambda_i} \).
	Hence, \( \sum_{i=1}^k g_{\lambda_i} = \sum_{i=1}^k a_{\lambda_i} \) holds if and only if \( g_{\lambda_i} = a_{\lambda_i} \) for all \( i \).
\end{proof}
